% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/metric_precision.R
\name{metric_precision}
\alias{metric_precision}
\title{metric_precision}
\usage{
metric_precision(
  actual,
  predicted,
  weight = rep(1, length(actual)),
  na.rm = FALSE,
  threshold = 0.5
)
}
\arguments{
\item{actual}{Array[Numeric] - Values we are aiming to predict.}

\item{predicted}{Array[Numeric] - Values that we have predicted.}

\item{weight}{Optional: Array[Numeric] - Weighting of predictions. If NULL even weighting is used}

\item{na.rm}{Optional: boolean - If \code{FALSE} function will return NA is any value in NA}

\item{threshold}{Optional: Numeric between 0 and 1. If prediction proablity is below \code{threshold} the predicted value is 0.}
}
\value{
precision of classification TP / (TP + FP)
}
\description{
Returns the precision TP / (TP + FP) of a classification using the confusion matrix
Note: Predictions should be annualized (independent of exposure)
Note: Perfect precision is 1, poor model is 0
}
\section{Inputs}{

}

\examples{

metric_precision(actual=c(0,1,0,0), predicted=c(0.1,0.9,0.4,0.6))

}
